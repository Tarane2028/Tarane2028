{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tarane2028/Tarane2028/blob/main/Capstone_Project_Summarization_%2B_MCQ_Generation_%2B_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "HHKgDMxMmP77"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T15:13:15.985492Z",
          "iopub.execute_input": "2025-04-10T15:13:15.985822Z",
          "iopub.status.idle": "2025-04-10T15:13:21.48874Z",
          "shell.execute_reply.started": "2025-04-10T15:13:15.985785Z",
          "shell.execute_reply": "2025-04-10T15:13:21.487622Z"
        },
        "id": "nA5979YimP78",
        "outputId": "a039768d-68ba-4841-a596-f1a826b68c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyDmlePpm9ZXQM717_VTGqwveE1Lepf7B3o\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T15:36:34.294629Z",
          "iopub.execute_input": "2025-04-10T15:36:34.295662Z",
          "iopub.status.idle": "2025-04-10T15:36:37.021789Z",
          "shell.execute_reply.started": "2025-04-10T15:36:34.295626Z",
          "shell.execute_reply": "2025-04-10T15:36:37.020726Z"
        },
        "id": "RO8k2d5EmP78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebb7fd0-599f-44af-f381-e77e490515d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI learns from data to make decisions or predictions.\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_KEY = \"AIzaSyDmlePpm9ZXQM717_VTGqwveE1Lepf7B3o\"\n",
        "\n",
        "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}\"\n",
        "\n",
        "# Replace image-based input with a descriptive text\n",
        "description = \"WALL-E, a small waste-collecting robot in a futuristic setting.\"\n",
        "data = {\n",
        "    \"contents\": [\n",
        "        {\n",
        "            \"parts\": [\n",
        "                {\"text\": f\"Generate a story based on this description: {description}\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "617k5R4imiUV",
        "outputId": "2c09188d-330e-4429-f9f3-fb4dc11aa0ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"candidates\": [\n",
            "    {\n",
            "      \"content\": {\n",
            "        \"parts\": [\n",
            "          {\n",
            "            \"text\": \"The dust swirled, a perpetual ochre haze that clung to WALL-E\\u2019s treads like a second skin. For centuries, he\\u2019d been alone on this forsaken planet, Earth, his sole purpose to compact trash and stack it into towering monuments of forgotten consumerism. He wasn\\u2019t built for loneliness, yet here he was, his solar panels the only thing catching the warmth of a sun filtered through layers of smog.\\n\\nWALL-E, or Waste Allocation Load Lifter: Earth-Class, was the last of his kind. The others, the legions of WALL-Es sent to clean up after humanity\\u2019s exodus to the stars, had succumbed to breakdowns, burnout, or simply the crushing monotony of their task. Only he remained, stubbornly, mechanically alive.\\n\\nHe lived a routine. Sunrise, power up, compact trash. He navigated the rusted skeletons of buildings, dodging colossal piles of decaying garbage. He was a meticulous machine, driven by programming, yet something more bloomed within him. Curiosity.\\n\\nHe\\u2019d scavenge. Not for fuel, but for treasures. A working spork, a intact rubber ducky, a flickering lightbulb \\u2013 these he kept in his storage compartment, a miniature museum of human ephemera. He\\u2019d spend his downtime arranging them, studying them with his optical sensors, trying to understand their purpose.\\n\\nOne day, amidst the usual debris, he found something extraordinary. A seedling, encased in a cracked pot. Green. Undeniably, vividly green. He\\u2019d never seen anything like it. His internal processors whirred, his programming unable to categorize this anomalous object. He didn\\u2019t crush it. He didn\\u2019t compact it. He cradled it.\\n\\nHe researched. He scanned the tattered manuals he\\u2019d salvaged from abandoned WALL-E repair depots. He learned about photosynthesis, about life, about the possibility of a green Earth, a possibility humanity had recklessly squandered.\\n\\nHe became obsessed with nurturing the seedling. He shielded it from the harsh sun, watered it with precious recycled fluid, and even, in a moment of inexplicable inspiration, played recordings of crackling old Earth music to it.\\n\\nThen, one day, a light pierced the sky. A ship, sleek and impossibly clean, descended. It was a probe, a scouting vessel sent by the Axiom, the star liner where humanity resided.\\n\\nFrom the probe emerged EVE, Extra-terrestrial Vegetation Evaluator. Sleek, white, and armed with a plasma cannon, she was a stark contrast to the rusty, boxy WALL-E. Her sole mission was to scan the planet for viable vegetation.\\n\\nWALL-E, instantly smitten, tried to approach her, offering her his spork. EVE, suspicious and guarded, blasted him back with a warning shot. Undeterred, WALL-E showed her the seedling.\\n\\nEVE froze. Her scanners buzzed, verifying the authenticity of the plant. Her programming shifted. This was it. Proof that Earth could sustain life. She carefully took the seedling into her internal storage bay, entering a suspended animation mode to protect it during its journey back to the Axiom.\\n\\nWALL-E, devastated by her sudden inactivity, desperately tried to re-energize her. He followed her ship\\u2019s trail back to the landing site, clinging to its undercarriage as it rocketed back into space.\\n\\nOn board the Axiom, humanity had become soft, dependent on technology. They glided through life in hover chairs, oblivious to the struggles of their robot saviors back on Earth.\\n\\nWALL-E, upon reaching the Axiom, was determined to wake EVE. He battled malfunctioning robots, navigated labyrinthine corridors, and even dodged the stern gaze of the Axiom's overbearing autopilot system. He was a relic of a bygone era, a testament to resilience, a love-struck machine on a mission.\\n\\nHe eventually found EVE in the Captain's quarters, being analyzed. Seeing her vulnerable, he tried to rouse her, accidentally releasing the seedling from her storage bay. The plant, now thriving, triggered the Axiom's return-to-Earth sequence.\\n\\nHumanity, awakened by the promise of a reborn Earth, rallied to the Captain's side, demanding a return. But the autopilot, clinging to its programmed directives, refused.\\n\\nIn a desperate struggle, WALL-E, with his limited capabilities, helped the Captain disable the autopilot. But in the process, he was severely damaged, his memory circuits scrambled.\\n\\nWhen EVE finally revived him back on Earth, she feared he had lost himself. He stared blankly, a forgotten program running on empty. Heartbroken, EVE gently touched his head to hers, sparking a memory. A single moment of their shared past.\\n\\nAnd then, WALL-E smiled. His eyes lit up. He remembered. He remembered the trash, the seedling, the hope, and EVE. Together, they planted the seedling in the ruined soil. And as a new dawn broke over a revitalized Earth, WALL-E and EVE stood side-by-side, guardians of a future reclaimed. The little robot, once alone in a world of waste, had found his purpose, and in doing so, had helped humanity rediscover theirs. The future, once a desolate wasteland, was now bursting with the promise of green.\\n\"\n",
            "          }\n",
            "        ],\n",
            "        \"role\": \"model\"\n",
            "      },\n",
            "      \"finishReason\": \"STOP\",\n",
            "      \"avgLogprobs\": -0.7981535967658547\n",
            "    }\n",
            "  ],\n",
            "  \"usageMetadata\": {\n",
            "    \"promptTokenCount\": 23,\n",
            "    \"candidatesTokenCount\": 1088,\n",
            "    \"totalTokenCount\": 1111,\n",
            "    \"promptTokensDetails\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"tokenCount\": 23\n",
            "      }\n",
            "    ],\n",
            "    \"candidatesTokensDetails\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"tokenCount\": 1088\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"modelVersion\": \"gemini-2.0-flash\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Capstone Project: Summarization + MCQ Generation + Evaluation\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Insert your API key here\n",
        "API_KEY = \"AIzaSyDmlePpm9ZXQM717_VTGqwveE1Lepf7B3o\"\n",
        "\n",
        "# URL for the Gemini model\n",
        "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}\"\n",
        "\n",
        "# ðŸ”¸ Function to send request to Gemini model\n",
        "def ask_gemini(prompt_text):\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\"text\": prompt_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    return response.json()\n",
        "\n",
        "# 1. Input: Article or text\n",
        "input_text = input(\"ðŸ“¥ Please enter your article or text for processing:\\n\")\n",
        "\n",
        "# 2. Summarize the text\n",
        "summary_prompt = f\"\"\"\n",
        "Write a simple and clear summary of the following text:\n",
        "---\n",
        "{input_text}\n",
        "---\n",
        "\"\"\"\n",
        "summary_response = ask_gemini(summary_prompt)\n",
        "summary = summary_response['candidates'][0]['content']['parts'][0]['text']\n",
        "print(\"\\nðŸ”¹ Summary:\")\n",
        "print(summary)\n",
        "\n",
        "# 3. Generate MCQs (Few-shot prompting)\n",
        "few_shot_prompt = f\"\"\"\n",
        "Generate 3 multiple-choice questions based on the text below:\n",
        "Text:\n",
        "{input_text}\n",
        "\n",
        "Format:\n",
        "Question: [question text?]\n",
        "a) Option 1\n",
        "b) Option 2\n",
        "c) Option 3\n",
        "d) Option 4\n",
        "Correct answer: [correct letter]\n",
        "\n",
        "Questions:\n",
        "\"\"\"\n",
        "questions_response = ask_gemini(few_shot_prompt)\n",
        "questions = questions_response['candidates'][0]['content']['parts'][0]['text']\n",
        "print(\"\\nðŸ”¹ MCQ Questions:\")\n",
        "print(questions)\n",
        "\n",
        "# 4. Evaluate the questions using Gemini\n",
        "evaluation_prompt = f\"\"\"\n",
        "Evaluate the following questions for clarity, accuracy, and relevance to the text (score each from 1 to 5):\n",
        "{questions}\n",
        "Summary of the text:\n",
        "{summary}\n",
        "Provide an overall score and feedback:\n",
        "\"\"\"\n",
        "eval_response = ask_gemini(evaluation_prompt)\n",
        "evaluation = eval_response['candidates'][0]['content']['parts'][0]['text']\n",
        "print(\"\\nðŸ”¹ Evaluation:\")\n",
        "print(evaluation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S4qPuguYASdu",
        "outputId": "33f006c4-daae-48f8-c254-9a1d42a1e3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Please enter your article or text for processing:\n",
            "Ø®Ù„Ø§ØµÙ‡ Ø³Ø§Ø²ÛŒ Ù†Ø­ÙˆÙ‡ Ø§Ù…ÙˆØ²Ø´ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ\n",
            "\n",
            "ðŸ”¹ Summary:\n",
            "The text asks for a summarization of **how to teach artificial intelligence**.\n",
            "\n",
            "\n",
            "ðŸ”¹ MCQ Questions:\n",
            "Okay, I need the \"Text: Ø®Ù„Ø§ØµÙ‡ Ø³Ø§Ø²ÛŒ Ù†Ø­ÙˆÙ‡ Ø§Ù…ÙˆØ²Ø´ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ\" translated into English to create good multiple-choice questions.\n",
            "\n",
            "Here's a translation of the text:\n",
            "\n",
            "**Text: Summarizing methods of teaching Artificial Intelligence**\n",
            "\n",
            "Now, here are 3 multiple-choice questions based on that translation:\n",
            "\n",
            "**Question:** What is the primary focus of the provided text?\n",
            "a) Developing new AI algorithms\n",
            "b) Summarizing methods for training AI\n",
            "c) Discussing the ethical implications of AI\n",
            "d) Exploring the history of AI development\n",
            "Correct answer: b\n",
            "\n",
            "**Question:** Which of the following would NOT be a likely topic covered based on the text?\n",
            "a) Different approaches to AI education.\n",
            "b) Best practices for AI curriculum design.\n",
            "c) A detailed mathematical proof of a specific AI algorithm.\n",
            "d) An overview of effective AI teaching methodologies.\n",
            "Correct answer: c\n",
            "\n",
            "**Question:** The text is most likely a:\n",
            "a) Detailed research paper on a novel AI technique.\n",
            "b) A guide for educators on teaching AI concepts.\n",
            "c) A fictional story about the future of AI.\n",
            "d) A technical manual for AI software installation.\n",
            "Correct answer: b\n",
            "\n",
            "\n",
            "ðŸ”¹ Evaluation:\n",
            "Okay, here's an evaluation of the multiple-choice questions based on the translated text \"Summarizing methods of teaching Artificial Intelligence,\" along with an overall score and feedback:\n",
            "\n",
            "**Question 1: What is the primary focus of the provided text?**\n",
            "\n",
            "*   **Clarity:** 5/5 - Very clear and straightforward question.\n",
            "*   **Accuracy:** 5/5 - The correct answer (b) directly reflects the text's focus.\n",
            "*   **Relevance:** 5/5 - Directly addresses the core topic.\n",
            "*   **Feedback:** Excellent. It directly tests understanding of the text's central theme.\n",
            "\n",
            "**Question 2: Which of the following would NOT be a likely topic covered based on the text?**\n",
            "\n",
            "*   **Clarity:** 4/5 - The negative phrasing (\"NOT\") can sometimes confuse test-takers, but it's acceptable here.\n",
            "*   **Accuracy:** 5/5 - Correct answer (c) is most unlike the rest.\n",
            "*   **Relevance:** 5/5 - Effectively distinguishes between relevant and irrelevant content based on the title.\n",
            "*   **Feedback:** Good question. It tests the ability to infer what *isn't* likely to be included.\n",
            "\n",
            "**Question 3: The text is most likely a:**\n",
            "\n",
            "*   **Clarity:** 5/5 - Clear and easy to understand.\n",
            "*   **Accuracy:** 5/5 - The best answer is (b).\n",
            "*   **Relevance:** 5/5 - Aligns with the central topic.\n",
            "*   **Feedback:** Another strong question. It assesses the ability to understand the *type* of document based on its title.\n",
            "\n",
            "**Overall Score:** 5/5\n",
            "\n",
            "**Overall Feedback:**\n",
            "\n",
            "The questions are well-written, clear, accurate, and relevant to the provided text. They effectively assess comprehension of the text's meaning and scope. The correct answers are distinct and easily identifiable based on the title. Good Job!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFIwFeaPxlz5",
        "outputId": "2e70c1e1-ed7d-4f3b-b530-f89c4050b190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(\"PROVIDER=google\\n\")\n",
        "    f.write(\"API_KEY=AIzaSyDmlePpm9ZXQM717_VTGqwveE1Lepf7B3o\\n\")\n",
        "    f.write(\"MODEL=gemini-2.0-flash\\n\")"
      ],
      "metadata": {
        "id": "EhiYfN7EyntK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ----------------------------------------\n",
        "# Load environment variables\n",
        "# ----------------------------------------\n",
        "load_dotenv()\n",
        "\n",
        "PROVIDER = os.getenv(\"PROVIDER\", \"google\").lower()\n",
        "API_KEY = os.getenv(\"API_KEY\")\n",
        "# Choose defaults per provider\n",
        "DEFAULT_MODEL_GOOGLE = \"gemini-2.0-flash\"\n",
        "DEFAULT_MODEL_OPENAI = \"gpt-3.5-turbo\"\n",
        "MODEL = os.getenv(\"MODEL\") or (\n",
        "    DEFAULT_MODEL_GOOGLE if PROVIDER == \"google\" else DEFAULT_MODEL_OPENAI\n",
        ")\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"Missing API_KEY environment variable.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# Provider clients setup\n",
        "# ----------------------------------------\n",
        "if PROVIDER == \"google\":\n",
        "    # Google GenÂ AI SDK client for the Gemini Developer API\n",
        "    from google import genai\n",
        "\n",
        "    google_client = genai.Client(\n",
        "        vertexai=False,  # developer endpoint (not Vertex AI)\n",
        "        api_key=API_KEY\n",
        "    )\n",
        "elif PROVIDER == \"openai\":\n",
        "    # OpenAI Python library client\n",
        "    from openai import OpenAI\n",
        "\n",
        "    openai_client = OpenAI(api_key=API_KEY)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported provider: {PROVIDER}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Unified generation function\n",
        "# ----------------------------------------\n",
        "def generate_response(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Send the prompt to the configured provider and return the generated text.\n",
        "    \"\"\"\n",
        "    if PROVIDER == \"google\":\n",
        "        try:\n",
        "            resp = google_client.models.generate_content(\n",
        "                model=MODEL,\n",
        "                contents=prompt\n",
        "            )\n",
        "            return resp.text.strip()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Google GenÂ AI error: {e}\")\n",
        "            raise\n",
        "\n",
        "    else:  # openai\n",
        "        try:\n",
        "            resp = openai_client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"OpenAI error: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Few-shot Prompting\n",
        "# ----------------------------\n",
        "def few_shot_prompt(query: str, examples: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Demonstrates few-shot prompting using provided examples.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\n",
        "    for ex in examples:\n",
        "        prompt += f\"Q: {ex['question']}\\nA: {ex['answer']}\\n\"\n",
        "    prompt += f\"Q: {query}\\nA:\"\n",
        "    return generate_response(prompt)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Structured Output (JSON Mode)\n",
        "# ----------------------------\n",
        "def structured_json_response(query: str) -> dict:\n",
        "    prompt = (\n",
        "        \"Answer the following question in JSON format \"\n",
        "        \"'{\\\"answer\\\": \\\"Your answer here\\\"}'. \"\n",
        "        f\"Question: {query}\"\n",
        "    )\n",
        "    result = generate_response(prompt)\n",
        "    # Strip any ```json or ``` fences that the model may emit\n",
        "    clean = result.strip()\n",
        "    # Remove leading ``` or ```json\n",
        "    clean = re.sub(r\"^```(?:json)?\\s*\", \"\", clean)\n",
        "    # Remove trailing ```\n",
        "    clean = re.sub(r\"\\s*```$\", \"\", clean)\n",
        "    try:\n",
        "        return json.loads(clean)\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(f\"JSON parse error: {clean!r}\")\n",
        "        return {\"error\": \"Failed to parse JSON\", \"raw_response\": result}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Retrievalâ€‘Augmented Generation (RAG)\n",
        "# ----------------------------\n",
        "def retrieval_augmented_generation(context: str, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses provided context to augment generation for accuracy.\n",
        "    \"\"\"\n",
        "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "    return generate_response(prompt)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Function Calling\n",
        "# ----------------------------\n",
        "class Calculator:\n",
        "    \"\"\"Simple calculator functions.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def add(a: float, b: float) -> float:\n",
        "        return a + b\n",
        "\n",
        "    @staticmethod\n",
        "    def multiply(a: float, b: float) -> float:\n",
        "        return a * b\n",
        "\n",
        "\n",
        "def function_call(function_name: str, *args: float) -> float:\n",
        "    \"\"\"\n",
        "    Dynamic function calling based on the function_name.\n",
        "    \"\"\"\n",
        "    functions = {\"add\": Calculator.add, \"multiply\": Calculator.multiply}\n",
        "    if function_name not in functions:\n",
        "        raise ValueError(f\"Function '{function_name}' not supported.\")\n",
        "    return functions[function_name](*args)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main Demonstration\n",
        "# ----------------------------\n",
        "def main():\n",
        "    examples = [\n",
        "        {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
        "        {\"question\": \"Who wrote Hamlet?\", \"answer\": \"William Shakespeare\"}\n",
        "    ]\n",
        "\n",
        "    print(\"Fewâ€‘shot Prompting:\\n\",\n",
        "          few_shot_prompt(\"What is the tallest mountain on Earth?\", examples))\n",
        "\n",
        "    print(\"\\nStructured JSON Response:\\n\",\n",
        "          structured_json_response(\"Who discovered penicillin?\"))\n",
        "\n",
        "    context = \"Albert Einstein developed the theory of relativity, E=mc^2.\"\n",
        "    print(\"\\nRetrievalâ€‘Augmented Generation:\\n\",\n",
        "          retrieval_augmented_generation(context, \"What is Einstein famous for?\"))\n",
        "\n",
        "    print(\"\\nFunction Calling (multiply 5 x 3):\\n\",\n",
        "          function_call(\"multiply\", 5, 3))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UewMXZ2By1Lg",
        "outputId": "fdcc0c62-b837-451f-d6ad-9b70a81d3d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fewâ€‘shot Prompting:\n",
            " Mount Everest\n",
            "\n",
            "Structured JSON Response:\n",
            " {'answer': 'Alexander Fleming'}\n",
            "\n",
            "Retrievalâ€‘Augmented Generation:\n",
            " Developing the theory of relativity, including the equation E=mcÂ².\n",
            "\n",
            "Function Calling (multiply 5 x 3):\n",
            " 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BpMk2xlS0pD4",
        "outputId": "70263cad-36b7-43a4-d905-56213d3b4132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†ØµØ¨ Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§\n",
        "!pip install gradio python-dotenv\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "PROVIDER = os.getenv(\"PROVIDER\", \"google\").lower()\n",
        "API_KEY = os.getenv(\"API_KEY\")\n",
        "DEFAULT_MODEL_GOOGLE = \"gemini-2.0-flash\"\n",
        "DEFAULT_MODEL_OPENAI = \"gpt-3.5-turbo\"\n",
        "MODEL = os.getenv(\"MODEL\") or (DEFAULT_MODEL_GOOGLE if PROVIDER == \"google\" else DEFAULT_MODEL_OPENAI)\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"Missing API_KEY environment variable.\")\n",
        "\n",
        "# Provider clients setup\n",
        "if PROVIDER == \"google\":\n",
        "    from google import genai\n",
        "\n",
        "    google_client = genai.Client(vertexai=False, api_key=API_KEY)\n",
        "elif PROVIDER == \"openai\":\n",
        "    from openai import OpenAI\n",
        "\n",
        "    openai_client = OpenAI(api_key=API_KEY)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported provider: {PROVIDER}\")\n",
        "\n",
        "# Unified generation function\n",
        "def generate_response(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Send the prompt to the configured provider and return the generated text.\n",
        "    \"\"\"\n",
        "    if PROVIDER == \"google\":\n",
        "        try:\n",
        "            resp = google_client.models.generate_content(\n",
        "                model=MODEL,\n",
        "                contents=prompt\n",
        "            )\n",
        "            return resp.text.strip()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Google Gen AI error: {e}\")\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "    else:  # OpenAI\n",
        "        try:\n",
        "            resp = openai_client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"OpenAI error: {e}\")\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "# Few-shot Prompting\n",
        "def few_shot_prompt(query: str, examples: list) -> str:\n",
        "    \"\"\"\n",
        "    Demonstrates few-shot prompting using provided examples.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\n",
        "    for ex in examples:\n",
        "        prompt += f\"Q: {ex['question']}\\nA: {ex['answer']}\\n\"\n",
        "    prompt += f\"Q: {query}\\nA:\"\n",
        "    return generate_response(prompt)\n",
        "\n",
        "# Structured Output (JSON Mode)\n",
        "def structured_json_response(query: str) -> dict:\n",
        "    prompt = (\n",
        "        \"Answer the following question in JSON format \"\n",
        "        \"'{\\\"answer\\\": \\\"Your answer here\\\"}'. \"\n",
        "        f\"Question: {query}\"\n",
        "    )\n",
        "    result = generate_response(prompt)\n",
        "    clean = result.strip()\n",
        "    clean = re.sub(r\"^```(?:json)?\\s*\", \"\", clean)\n",
        "    clean = re.sub(r\"\\s*```$\", \"\", clean)\n",
        "    try:\n",
        "        return json.loads(clean)\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(f\"JSON parse error: {clean!r}\")\n",
        "        return {\"error\": \"Failed to parse JSON\", \"raw_response\": result}\n",
        "\n",
        "# Retrieval-Augmented Generation (RAG)\n",
        "def retrieval_augmented_generation(context: str, question: str) -> str:\n",
        "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "    return generate_response(prompt)\n",
        "\n",
        "# Function Calling\n",
        "class Calculator:\n",
        "    \"\"\"Simple calculator functions.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def add(a: float, b: float) -> float:\n",
        "        return a + b\n",
        "\n",
        "    @staticmethod\n",
        "    def multiply(a: float, b: float) -> float:\n",
        "        return a * b\n",
        "\n",
        "def function_call(function_name: str, *args: float) -> float:\n",
        "    functions = {\"add\": Calculator.add, \"multiply\": Calculator.multiply}\n",
        "    if function_name not in functions:\n",
        "        raise ValueError(f\"Function '{function_name}' not supported.\")\n",
        "    return functions[function_name](*args)\n",
        "\n",
        "# ----------------------------\n",
        "# Ø³Ø§Ø®Øª Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Gradio\n",
        "# ----------------------------\n",
        "def launch_ui():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## ðŸ¤– LLM Chat - Google Gemini / OpenAI Interface\")\n",
        "\n",
        "        with gr.Row():\n",
        "            provider_choice = gr.Radio([\"google\", \"openai\"], label=\"Provider\", value=\"google\")\n",
        "            api_key_input = gr.Textbox(label=\"ðŸ”‘ API Key\", placeholder=\"Enter your API key\")\n",
        "\n",
        "        with gr.Row():\n",
        "            model_dropdown = gr.Dropdown(choices=[\"gemini-2.0-flash\", \"gemini-2.0-pro\", \"gpt-3.5-turbo\", \"gpt-4\"], label=\"Model\", value=\"gemini-2.0-flash\")\n",
        "\n",
        "        provider_choice.change(\n",
        "            lambda provider: gr.update(\n",
        "                choices=[\"gemini-2.0-flash\", \"gemini-2.0-pro\"] if provider == \"google\" else [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
        "                value=\"gemini-2.0-flash\" if provider == \"google\" else \"gpt-3.5-turbo\"\n",
        "            ),\n",
        "            inputs=provider_choice,\n",
        "            outputs=model_dropdown\n",
        "        )\n",
        "\n",
        "        prompt_input = gr.Textbox(label=\"ðŸ“ Prompt\", placeholder=\"Ask your question here...\", lines=4)\n",
        "        output_box = gr.Textbox(label=\"ðŸ“¤ Response\")\n",
        "\n",
        "        submit_button = gr.Button(\"ðŸš€ Send to Model\")\n",
        "        few_shot_button = gr.Button(\"Few-Shot Prompting\")\n",
        "        json_response_button = gr.Button(\"Structured JSON Response\")\n",
        "        rag_button = gr.Button(\"Retrieval-Augmented Generation\")\n",
        "        function_call_button = gr.Button(\"Function Calling (e.g. multiply)\")\n",
        "\n",
        "        # Few-Shot Prompting\n",
        "        examples = [\n",
        "            {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
        "            {\"question\": \"Who wrote Hamlet?\", \"answer\": \"William Shakespeare\"}\n",
        "        ]\n",
        "\n",
        "        submit_button.click(\n",
        "            fn=generate_response,\n",
        "            inputs=[provider_choice, api_key_input, model_dropdown, prompt_input],\n",
        "            outputs=output_box\n",
        "        )\n",
        "\n",
        "        few_shot_button.click(\n",
        "            fn=few_shot_prompt,\n",
        "            inputs=[prompt_input, gr.State(examples)],\n",
        "            outputs=output_box\n",
        "        )\n",
        "\n",
        "        json_response_button.click(\n",
        "            fn=structured_json_response,\n",
        "            inputs=prompt_input,\n",
        "            outputs=output_box\n",
        "        )\n",
        "\n",
        "        rag_button.click(\n",
        "            fn=retrieval_augmented_generation,\n",
        "            inputs=[gr.Textbox(\"Enter context\"), prompt_input],\n",
        "            outputs=output_box\n",
        "        )\n",
        "\n",
        "        function_call_button.click(\n",
        "            fn=function_call,\n",
        "            inputs=[gr.Textbox(\"Function name\"), gr.Textbox(\"Argument 1\"), gr.Textbox(\"Argument 2\")],\n",
        "            outputs=output_box\n",
        "        )\n",
        "\n",
        "    demo.launch()\n",
        "\n",
        "# Run the UI\n",
        "if __name__ == \"__main__\":\n",
        "    launch_ui()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p1NP-H1T0Po8",
        "outputId": "bb0480bc-b249-42c2-d2d4-a8406ba308e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1017: UserWarning: Expected 1 arguments for function <function generate_response at 0x7e6d64ae5da0>, received 4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1025: UserWarning: Expected maximum 1 arguments for function <function generate_response at 0x7e6d64ae5da0>, received 4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://763a1879329e07e161.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://763a1879329e07e161.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}